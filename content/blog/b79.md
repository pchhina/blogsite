---
title: "Introduction to Pandas"
date: 2021-05-06T18:27:39-05:00
draft: false
tags: ["Python", "Data Science"]
---

- how to store and manipulate single-dimensional indexed data in series object
- series - a mix of list and dictionary, like a 2-column array. col1 = index, col2 = data

```python
import pandas as pd
```
underneath, Series is a typed array from Numpy.
None is implicitly converted to NaN if list is numbers, and type (of Series object) become `float64` because Pandas represents NaN as floating point number.
```python
pd.Series([1,2])
pd.Series([1,2,None])
```
So if integer list is cast as float64, it means there was some missing data. how to check presece of NaN:
```python
import numpy as np
np.isnan(np.nan)
```
`.count()` method ignores `NaN` values, so we can find how many values are `NaN`s:
```python
x = pd.Series([1,2,None])
x.count()/len(x)
```
like list, dictionary can be converted to Series:
```python
grades = {"Mike": "A", "Anna": "B", "Charlie": "C"}
s = pd.Series(grades)
s.index

```
indices can be specified separately as list:
```python
s = pd.Series([1,2,3], index = ['first', 'second', 'third'])
```
if index does not exist in dictionary key, the value of that index will be NaN (or None) and only the keys listed in index will be part of the Series.
### Series Querying
- use iloc to query by numeric index and loc to query by index label
```python
s.iloc[1]
# or s[1]
s.loc['third']
# or s['third']
# but iloc and loc is safer (e.g. avoids confusion when index is an integer)
```

### Data transformation
```python
acidity = pd.Series([3.1, 2.5, 3.5, 1.9])
np.sum(acidity)/len(acidity)
```
- faster (1 to 2 orders of magnitude) than loops
```python
acidities = pd.Series(np.random.rand(100000)*10)
```
```python
%%timeit -n 100
sum = 0
for acidity in acidities:
    sum += acidity
sum/len(acidities)
```
```python
%%timeit -n 100
np.sum(acidities)/len(acidities)
```
- broadcasting can update values in place
```python
# subtract 1 from acidities
acidities -= 1
acidities.head()
```

- indices can be of mixed types and can repeat
```python
temps = pd.Series({'high':70, 'low':30})
humidity = pd.Series({1: 'low', 2: 'high'})
temps.append(humidity)

```

## Dataframes
- think of dataframe as 2-axis labeled array (names in both can be non-unique)
- can be sliced similar to Series using loc and iloc - use loc/iloc for row selection and indexing for column selection, e.g. `df['col1']` will get all rows of 'col1' column, will be returned as Series
- can be transposed using .T
- columns always have a name
- operations can be chained because output is either a Series or DataFrame
```python
df.loc['rowName']['ColName']
```
- chaining should be avoided if possible because it returns a copy of the dataframe which can be costly
- data can be removed using `drop` which also returns a copy of the dataframe
```python
df.drop('rowName') # drops the row but returns a copy, but can be changed using inplace and axis
dfCopy = df.copy()
dfCopy.drop('colName', inplace=True, axis=1) # now it drops the column and does it in-place
del dfCopy['colName'] # drops the column too inplace
```

- new columns can be added using assignment operator
```python
df['newCol'] = None # assigns all row values to None
```
## DataFrame Indexing and Loading

### reading from CSV
```python
df = pd.read_csv(<file string>)
```
- columns can be renamed by passing dictionary of old(keys) and new(value)
- columns attrubute to get a list of column names
```python
df_new = df.rename(columns={'old1':'new1', 'old3':'new3'})
```
- rename does not changes the original dataframe
- column names can be stripped off of whitespaces/tabs using `mapper` attribute
```python
df_new = df.rename(mapper=str.strip, axis=1)
```
- column names can be transformed efficiently using lists
```python
cols = list(df.columns) # convert to list
cols = [x.lower().strip() for x in cols] # using list comprehension to transform
df.columns = cols # assigning the new list back to column names
```

## Querying a DataFrame
- boolean mask can be created and applied to filter rows conditionally based on column values
- to create a mask, use indexing operator to get the column and comparison operator(s) to broadcast the comparison over entire column
```python
mask = df['col1'] > 50
```
- use indexing operator again to apply the mask
```python
df[mask]
```
- use `&` and `|` to combine multiple booleans
```python
mask2 = df['col1'] > 50 & df['col2'] == 1
df[mask2]
```
- rows containing na values can be dropped using `dropna()` method
- for getting one column back, pass the column name as string to indexing operator
- for getting multiple columns, use a list of column names
```python
df['col1'] # get col1 
df[['col1', 'col2']] # get col1 and col2
```
## How indexes work?
Index is a row elevel label (rows correspond to axis = 0). Indices are auto-generated if not set explicitly. `set_index()` can be used to set indices that takes list of columns and promotes those columns to an index. This function is destructive, i.e., it doesn't keep the current index. To preserve:
```python
df['Serial Number'] = df.index # copy the indexed data into its own column
df = df.set_index('Chance of Admit') # set the index to another column
df.head()
```
To get rid of index completely, use `reset_index()`
```python
df = df.reset.index()
df.head()
```
Multi-level indexing is possible similat to composite keys in sql database:

```python
df = pd.read_csv("census.csv")
df.head()
df['SUMLEV'].unique() # unique values in a column
df = df[df['SUMLEV'] == 50] # generate new dataframe using a mask
df.head()
cols_to_keep = ['STNAME', 'CTYNAME', 'BIRTHS2010', 'BIRTHS2011', 'BIRTHS2012', 'BIRTHS2013', 'BIRTHS2014', 'BIRTHS2015', 'POPESTIMATE2010', 'POPESTIMATE2011', 'POPESTIMATE2012','POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015']
df = df[cols_to_keep]
df.head()
# create multi-index using state and county name
df = df.set_index(['STNAME', 'CTYNAME'])
df.head()
df.loc['Michigan', 'Washtenaw County'] # loc can take multiple attributes; for multiindex, args are provided in order
# tuples can be provided for accessing more than one multi-key
df.loc[[('Michigan', 'Wahtenaw County'), ('Michigan', 'Wayne County')]]
```
## Missing Values
- missing value can be an omission, called **Missing at Random** if other variables can be used to predict this data and **Missing Completely at Random (MCAR)** if there is no correlation with other variables and thus the missing value cannot be predicted.

```python
df = pd.read_csv('class_grades.csv')
df.head()
mask = df.isnull() # broadcast isnull to the whole dataframe
mask.head()
df.dropna().head(10) # rows with one or more na values are dropped
df.fillna(0, inplace=True) # fill missing values with 0, don't create a copy
```
`ffill` and `bfill` can be used to fill missing values in an ordered data. `ffill` will replace missing with the last valid value while `bfill` will replace with the next valid value. If data is not ordered, one can use indexing to order it before applying ffill and bfill.

`replace()` can be used to replace values explicitly or with regex match

statistical transformation on dataframes typically ignore missing values.
